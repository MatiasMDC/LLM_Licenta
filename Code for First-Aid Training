#!/usr/bin/env python3
import os
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model

def main():
    root           = "/mnt/d/llama_wsl"
    base_model_dir = os.path.join(root, "models", "tinyllama-merged")
    output_dir     = os.path.join(root, "models", "tinyllama-firstaid-lora-r16")

    os.makedirs(output_dir, exist_ok=True)

    # 1) Load merged model
    tokenizer = AutoTokenizer.from_pretrained(base_model_dir, local_files_only=True)
    model = AutoModelForCausalLM.from_pretrained(
        base_model_dir,
        local_files_only=True,
        load_in_8bit=True,
        device_map="auto",
    )

    # 2) New LoRA on top, r=16
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.10,
        bias="none",
    )
    model = get_peft_model(model, lora_config)

    # 3) Dataset prep (same as before)
    ds = load_dataset("lextale/FirstAidInstructionsDataset")["train"]
    def preprocess(ex):
        system = "You are an expert first-aid assistant. Answer with clear, numbered, step-by-step instructions.\n\n"
        prompt = f"{system}Q: {ex['question']}\nA:"
        tok    = tokenizer(prompt + ex["answer"], truncation=True, max_length=512)
        return {"input_ids": tok["input_ids"], "attention_mask": tok["attention_mask"], "labels": tok["input_ids"]}
    ds = ds.map(preprocess, batched=False)

    # 4) Training args
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        num_train_epochs=20,      # new pass
        learning_rate=1e-4,
        optim="paged_adamw_8bit",
        save_steps=len(ds),       # one checkpoint per epoch
        save_total_limit=2,
        logging_steps=20,
        report_to="none",
    )

    # 5) Train (fresh, no resume) & save
    trainer = Trainer(model=model, args=training_args, train_dataset=ds)
    trainer.train()
    trainer.save_model(output_dir)
    print("âœ… Completed r=16 LoRA fine-tune at", output_dir)

if __name__ == "__main__":
    main()
